{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월간 데이콘 발화자의 감정인식 AI 경진대회\n",
    "\n",
    "- Baseline code를 기준으로 작성되었습니다.  \n",
    "\n",
    "- emoberta-large 모델, StratifiedKFold를 적용하고, 문맥이나 발화 순서를 고려하지 않고 학습시키는 방향으로 수행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FA979Gf3NO4P",
    "outputId": "a8700193-a78a-4e4d-f034-8474aeb5d68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 11 01:29:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    26W / 250W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgJfWRB9QleO"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fzvQU3lAdnm_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9-jvQLzANMbv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SO7GM91VQzgA"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiZ6FYECQ93c"
   },
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "05nBnq4EfSs7"
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS': 5,\n",
    "    'LEARNING_RATE': 1e-6,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'SEED': 42,\n",
    "    'PLM': \"tae898/emoberta-large\",\n",
    "    'OPTIMIZER': \"Adam\",\n",
    "    'split': '8-2',\n",
    "    'NFOLD': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPubv2UZRRpv"
   },
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9uNDC9vlRX22"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mtvfLD_RfHs"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "agclDVmXQ1IV",
    "outputId": "ac773437-d919-4c75-80ab-9636fe405df5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>TRAIN_9984</td>\n",
       "      <td>You or me?</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>TRAIN_9985</td>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>TRAIN_9986</td>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1038</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>TRAIN_9987</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>All</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>TRAIN_9988</td>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1038</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                          Utterance   Speaker  \\\n",
       "9984  TRAIN_9984                                         You or me?  Chandler   \n",
       "9985  TRAIN_9985  I got it. Uh, Joey, women don't have Adam's ap...      Ross   \n",
       "9986  TRAIN_9986               You guys are messing with me, right?      Joey   \n",
       "9987  TRAIN_9987                                              Yeah.       All   \n",
       "9988  TRAIN_9988  That was a good one. For a second there, I was...      Joey   \n",
       "\n",
       "      Dialogue_ID    Target  \n",
       "9984         1038   neutral  \n",
       "9985         1038   neutral  \n",
       "9986         1038  surprise  \n",
       "9987         1038   neutral  \n",
       "9988         1038       joy  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/MyFiles/DACON/Speaker_emotion/data/\"\n",
    "data = pd.read_csv(path + 'train.csv')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speaker 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>TRAIN_9984</td>\n",
       "      <td>You or me?</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>TRAIN_9985</td>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>TRAIN_9986</td>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>1038</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>TRAIN_9987</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>TRAIN_9988</td>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>1038</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                          Utterance  \\\n",
       "0     TRAIN_0000  also I was the point person on my company’s tr...   \n",
       "1     TRAIN_0001                   You must’ve had your hands full.   \n",
       "2     TRAIN_0002                            That I did. That I did.   \n",
       "3     TRAIN_0003      So let’s talk a little bit about your duties.   \n",
       "4     TRAIN_0004                             My duties?  All right.   \n",
       "...          ...                                                ...   \n",
       "9984  TRAIN_9984                                         You or me?   \n",
       "9985  TRAIN_9985  I got it. Uh, Joey, women don't have Adam's ap...   \n",
       "9986  TRAIN_9986               You guys are messing with me, right?   \n",
       "9987  TRAIN_9987                                              Yeah.   \n",
       "9988  TRAIN_9988  That was a good one. For a second there, I was...   \n",
       "\n",
       "      Dialogue_ID    Target  \n",
       "0               0   neutral  \n",
       "1               0   neutral  \n",
       "2               0   neutral  \n",
       "3               0   neutral  \n",
       "4               0  surprise  \n",
       "...           ...       ...  \n",
       "9984         1038   neutral  \n",
       "9985         1038   neutral  \n",
       "9986         1038  surprise  \n",
       "9987         1038   neutral  \n",
       "9988         1038       joy  \n",
       "\n",
       "[9989 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = data[[\"ID\", \"Utterance\", \"Dialogue_ID\", \"Target\"]]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     4710\n",
       "joy         1743\n",
       "surprise    1205\n",
       "anger       1109\n",
       "sadness      683\n",
       "disgust      271\n",
       "fear         268\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SHP0V-mNMbz"
   },
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xZrfnW6DNMbz"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le = le.fit(train_ds['Target'])\n",
    "train_ds['Target']=le.transform(train_ds['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CLrmSQ_9OqAb",
    "outputId": "a5ea1b1c-5bc4-48ae-b299-7e9f118f3c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> anger\n",
      "1 -> disgust\n",
      "2 -> fear\n",
      "3 -> joy\n",
      "4 -> neutral\n",
      "5 -> sadness\n",
      "6 -> surprise\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(le.classes_):\n",
    "    print(i, '->', label)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_5839</td>\n",
       "      <td>Can I just say, I really admire what you’re do...</td>\n",
       "      <td>619</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_3058</td>\n",
       "      <td>Look, things like last night they don’t just h...</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_8746</td>\n",
       "      <td>Damn! I thought that was going to be romantic ...</td>\n",
       "      <td>919</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_2664</td>\n",
       "      <td>I’m wearing his briefs right now.</td>\n",
       "      <td>281</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0035</td>\n",
       "      <td>What, what, what?!</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>TRAIN_5734</td>\n",
       "      <td>I was</td>\n",
       "      <td>610</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>TRAIN_5191</td>\n",
       "      <td>Okay!</td>\n",
       "      <td>551</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>TRAIN_5390</td>\n",
       "      <td>Don’t you talk to my husband like that you stu...</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>TRAIN_0860</td>\n",
       "      <td>This sucks! I didn’t know I had to stay up all...</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>TRAIN_7270</td>\n",
       "      <td>I mean, there’s no way I can make myself talle...</td>\n",
       "      <td>771</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                          Utterance  \\\n",
       "0     TRAIN_5839  Can I just say, I really admire what you’re do...   \n",
       "1     TRAIN_3058  Look, things like last night they don’t just h...   \n",
       "2     TRAIN_8746  Damn! I thought that was going to be romantic ...   \n",
       "3     TRAIN_2664                  I’m wearing his briefs right now.   \n",
       "4     TRAIN_0035                                 What, what, what?!   \n",
       "...          ...                                                ...   \n",
       "9984  TRAIN_5734                                              I was   \n",
       "9985  TRAIN_5191                                              Okay!   \n",
       "9986  TRAIN_5390  Don’t you talk to my husband like that you stu...   \n",
       "9987  TRAIN_0860  This sucks! I didn’t know I had to stay up all...   \n",
       "9988  TRAIN_7270  I mean, there’s no way I can make myself talle...   \n",
       "\n",
       "      Dialogue_ID  Target  \n",
       "0             619       4  \n",
       "1             320       4  \n",
       "2             919       6  \n",
       "3             281       4  \n",
       "4               3       6  \n",
       "...           ...     ...  \n",
       "9984          610       6  \n",
       "9985          551       4  \n",
       "9986          572       0  \n",
       "9987           89       0  \n",
       "9988          771       4  \n",
       "\n",
       "[9989 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffling하고 index reset\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_5839</td>\n",
       "      <td>Can I just say, I really admire what you’re do...</td>\n",
       "      <td>619</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_3058</td>\n",
       "      <td>Look, things like last night they don’t just h...</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_8746</td>\n",
       "      <td>Damn! I thought that was going to be romantic ...</td>\n",
       "      <td>919</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_2664</td>\n",
       "      <td>I’m wearing his briefs right now.</td>\n",
       "      <td>281</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0035</td>\n",
       "      <td>What, what, what?!</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                          Utterance  Dialogue_ID  \\\n",
       "0  TRAIN_5839  Can I just say, I really admire what you’re do...          619   \n",
       "1  TRAIN_3058  Look, things like last night they don’t just h...          320   \n",
       "2  TRAIN_8746  Damn! I thought that was going to be romantic ...          919   \n",
       "3  TRAIN_2664                  I’m wearing his briefs right now.          281   \n",
       "4  TRAIN_0035                                 What, what, what?!            3   \n",
       "\n",
       "   Target  Kfold  \n",
       "0       4      1  \n",
       "1       4      2  \n",
       "2       6      2  \n",
       "3       4      4  \n",
       "4       6      3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG['NFOLD'], shuffle=True, random_state=CFG['SEED'])\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=train_ds, y=train_ds.Target)):\n",
    "    train_ds.loc[val_ , \"Kfold\"] = int(fold)\n",
    "\n",
    "train_ds[\"Kfold\"] = train_ds[\"Kfold\"].astype(int)\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG['NFOLD'], CFG['EPOCHS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erqF3l3LSFx7"
   },
   "source": [
    "## Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2PcTjaXGRrdG",
    "outputId": "5aaf454e-23da-4947-ba09-0d4d1fb568ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8562\n",
      "1427\n"
     ]
    }
   ],
   "source": [
    "train_df = train_ds[train_ds.Kfold != fold].reset_index(drop=True)\n",
    "valid_df = train_ds[train_ds.Kfold == fold].reset_index(drop=True)\n",
    "\n",
    "train_len=len(train_df)\n",
    "val_len=len(valid_df)\n",
    "\n",
    "print(train_len)\n",
    "print(val_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3W96rVUTaaq"
   },
   "source": [
    "## Tokenizer Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ItgoMXv4TXX9"
   },
   "outputs": [],
   "source": [
    "tokenizers = AutoTokenizer.from_pretrained(CFG[\"PLM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fixY8ozVT60m"
   },
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "oNfNtFJlmScu"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, data, mode = \"train\"):\n",
    "        self.dataset = data\n",
    "        self.tokenizer = tokenizers\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset['Utterance'][idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "    \n",
    "        if self.mode == \"train\":\n",
    "            y = self.dataset['Target'][idx]\n",
    "            return input_ids, attention_mask, y\n",
    "        \n",
    "        else:\n",
    "            return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "26ZtQWpJqGxb"
   },
   "outputs": [],
   "source": [
    "train = CustomDataset(train_df, mode = \"train\")\n",
    "valid = CustomDataset(valid_df, mode = \"train\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size= CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid, batch_size= CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZeXapo-Ugye"
   },
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GxgcbzKaoSc7"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5, num_classes=len(le.classes_)):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(CFG[\"PLM\"])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,\n",
    "                                     return_dict=False\n",
    "                                    )\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEM3ikH6U4gt"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fLmC6DvAU6HG"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, device, fold=CFG[\"NFOLD\"]):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = \"None\"\n",
    "    for epoch_num in range(CFG[\"EPOCHS\"]):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for input_ids, attention_mask, train_label in tqdm(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            input_id = input_ids.to(device)\n",
    "            mask = attention_mask.to(device)\n",
    "\n",
    "            output = model(input_id, mask)     \n",
    "    \n",
    "            batch_loss = criterion(output, train_label.long()) \n",
    "            train_loss.append(batch_loss.item())\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_score = validation(model, criterion, test_loader, device)\n",
    "        print(f'Epoch [{epoch_num}] of {fold}th Fold, Train Loss : [{np.mean(train_loss) :.5f}] Val Loss : [{np.mean(val_loss) :.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "\n",
    "        if best_score < val_score:\n",
    "            best_model = model\n",
    "            best_score = val_score\n",
    "            torch.save(model.state_dict(), os.path.join(RECORDER_DIR, f\"best_model-Fold-{fold}.pt\"))\n",
    "        \n",
    "    return best_model                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "r2WA2WfWW1eC"
   },
   "outputs": [],
   "source": [
    "def competition_metric(true, pred):\n",
    "    return f1_score(true, pred, average=\"macro\")\n",
    "\n",
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    model_preds = []\n",
    "    true_labels = []  \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, valid_label in tqdm(test_loader):\n",
    "            \n",
    "            valid_label = valid_label.to(device)\n",
    "            input_id = input_ids.to(device)\n",
    "            mask = attention_mask.to(device)\n",
    "\n",
    "            output = model(input_id, mask)     \n",
    "    \n",
    "            batch_loss = criterion(output, valid_label.long()) \n",
    "            val_loss.append(batch_loss.item())      \n",
    "            \n",
    "            model_preds += output.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += valid_label.detach().cpu().numpy().tolist()\n",
    "        val_f1 = competition_metric(true_labels, model_preds)\n",
    "    return val_loss, val_f1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXDUw6peYSJe"
   },
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ToqY4igbNMb2"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "# 시간 고유값 \n",
    "PROJECT_DIR = './'\n",
    "os.chdir(PROJECT_DIR)\n",
    "kst = timezone(timedelta(hours=9))        \n",
    "train_serial = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 기록 경로\n",
    "RECORDER_DIR = os.path.join(PROJECT_DIR, 'results', train_serial)\n",
    "# 현재 시간 기준 폴더 생성\n",
    "os.makedirs(RECORDER_DIR, exist_ok=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KOUGrLZLYTW9",
    "outputId": "74b6eafc-ce34-47e3-ba1c-13661dc1ba98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold: 0 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1071/1071 [15:07<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] of 0th Fold, Train Loss : [1.05248] Val Loss : [0.78328] Val F1 Score : [0.51210]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:05<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] of 0th Fold, Train Loss : [0.79200] Val Loss : [0.76008] Val F1 Score : [0.55343]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:05<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] of 0th Fold, Train Loss : [0.72169] Val Loss : [0.75015] Val F1 Score : [0.64240]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:05<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3] of 0th Fold, Train Loss : [0.66617] Val Loss : [0.74237] Val F1 Score : [0.65562]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:06<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4] of 0th Fold, Train Loss : [0.60494] Val Loss : [0.75199] Val F1 Score : [0.66331]\n",
      "======== Fold: 1 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1071/1071 [15:06<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] of 1th Fold, Train Loss : [1.03196] Val Loss : [0.78223] Val F1 Score : [0.53664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:05<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] of 1th Fold, Train Loss : [0.78709] Val Loss : [0.73613] Val F1 Score : [0.64157]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 345/1071 [04:51<10:14,  1.18it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1071/1071 [15:06<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] of 6th Fold, Train Loss : [1.01972] Val Loss : [0.76816] Val F1 Score : [0.50725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:06<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] of 6th Fold, Train Loss : [0.78568] Val Loss : [0.76635] Val F1 Score : [0.62561]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:06<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] of 6th Fold, Train Loss : [0.71192] Val Loss : [0.73057] Val F1 Score : [0.66525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:07<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3] of 6th Fold, Train Loss : [0.65489] Val Loss : [0.73549] Val F1 Score : [0.68189]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [15:06<00:00,  1.18it/s]\n",
      "100%|██████████| 179/179 [00:49<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4] of 6th Fold, Train Loss : [0.60812] Val Loss : [0.74850] Val F1 Score : [0.66108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fold in range(0, CFG['NFOLD']):\n",
    "    print(f\"======== Fold: {fold} =========\")\n",
    "\n",
    "    model = BaseModel()\n",
    "    # model.eval()\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "\n",
    "    infer_model = train(model, optimizer, train_dataloader, val_dataloader, device, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnMKAC2AZBJu"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f'/MyFiles/DACON/Speaker_emotion/results/{train_serial}/'\n",
    "print(base_path)\n",
    "\n",
    "model_paths = [\n",
    "    base_path + \"best_model-Fold-0.pt\",\n",
    "    base_path + \"best_model-Fold-1.pt\",\n",
    "    base_path + \"best_model-Fold-2.pt\",\n",
    "    base_path + \"best_model-Fold-3.pt\",\n",
    "    base_path + \"best_model-Fold-4.pt\",\n",
    "    base_path + \"best_model-Fold-5.pt\",\n",
    "    base_path + \"best_model-Fold-6.pt\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YQlIB9VJhN1A"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(path + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eHNjYWdAZSyC"
   },
   "outputs": [],
   "source": [
    "test = CustomDataset(test, mode = \"test\")\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size= 10, #CFG['BATCH_SIZE'], \n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3QE1wphCZS0p"
   },
   "outputs": [],
   "source": [
    "def inference(model_paths, test_loader, device):\n",
    "\n",
    "    test_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, path in enumerate(model_paths):  \n",
    "            test_predict = []\n",
    "            \n",
    "            model = BaseModel().to(device)\n",
    "            model.load_state_dict(torch.load(path))\n",
    "            model.eval()\n",
    "\n",
    "            print(f\"Prediction for model {i+1}\")\n",
    "            for input_ids, attention_mask in tqdm(test_loader):\n",
    "                input_id = input_ids.to(device)\n",
    "                mask = attention_mask.to(device)\n",
    "                y_pred = model(input_id, mask)\n",
    "                test_predict.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "            test_predict1 = np.concatenate(np.array(test_predict), axis = 0) # test_predict1: [total_bs, 7]\n",
    "            print(test_predict1.shape)\n",
    "            \n",
    "            test_predicts.append(test_predict1) # test_predicts: [[total_bs, 7],  [total_bs, 7],  .... ]\n",
    "\n",
    "    test_predicts_final = np.mean(test_predicts, axis=0)\n",
    "    \n",
    "    return test_predicts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pd79nHhsZS3I",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-large were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for model 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:25<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2610, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = inference(model_paths, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = np.argmax(preds, axis = 1)\n",
    "N_preds = le.inverse_transform(n_preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcWQHa_GZ4jz"
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "VcwcJY4wZS8U"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Target\n",
       "0  TEST_0000       0\n",
       "1  TEST_0001       0\n",
       "2  TEST_0002       0\n",
       "3  TEST_0003       0\n",
       "4  TEST_0004       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv(path + 'sample_submission.csv')\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    Target\n",
       "0  TEST_0000  surprise\n",
       "1  TEST_0001   neutral\n",
       "2  TEST_0002   neutral\n",
       "3  TEST_0003   neutral\n",
       "4  TEST_0004       joy"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['Target'] = N_preds\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ImZHkRxVNMb3"
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f\"/MyFiles/DACON/Speaker_emotion/results/submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "175f0a38a62dc0beb1e17a90921ab8d7bbf773d127689593e2caaacc63115b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
